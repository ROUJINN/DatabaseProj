他的要求是请部署Hadoop集群（单机模式、伪分布式模式、完全分布式模式均可）

在 WSL2 (Windows Subsystem for Linux) 环境下运行这个项目是一个非常好的选择。WSL2 本质上是一个 Linux (通常是 Ubuntu) 环境，非常适合开发和运行这类 Python/MySQL/Hadoop 项目。

以下是针对 WSL2 环境的**从零开始详细操作指南**，包含了你需要安装的所有软件和具体命令。

### 准备工作：检查与安装基础软件

在开始之前，我们需要安装 **Python**、**MySQL** 和 **Java/Hadoop** (针对第4步)。

打开你的 WSL2 终端（Terminal），依次执行以下命令：

#### 1. 更新软件源并安装 Python 和 pip
```bash
sudo apt update
sudo apt install python3 python3-pip -y
```

#### 2. 安装 MySQL 数据库
```bash
sudo apt install mysql-server -y
# 启动 MySQL 服务
sudo service mysql start
```

#### 3. (针对第4步) 安装 Java (Hadoop 依赖 Java)
如果你必须运行真实的 Hadoop MapReduce，你需要安装 Java：
```bash
sudo apt install default-jdk -y
# 检查是否安装成功
java -version
```

---

### 正式步骤：按照项目要求操作

假设你的项目代码文件夹已经在 WSL 里了（例如在 `~/my_project`）。请先通过 `cd` 命令进入项目根目录：
```bash
cd /path/to/your/project  # 把这里替换成你实际的项目路径
```

#### 第一步：安装 Python 依赖

直接在终端运行：
```bash
pip3 install flask pymysql faker
```

#### 第二步：初始化数据库

这一步比较关键，WSL2 下的 MySQL 默认 root 用户不需要密码（使用 sudo 登录），但为了配合你的 Python 代码，我们需要设置一下。

1.  **登录 MySQL**：
    ```bash
    sudo mysql
    ```

2.  **创建数据库和表** (在 MySQL 命令行 `mysql>` 中执行)：
    *   你可以直接复制 `sql/schema.sql` 的内容粘贴进去。
    *   或者使用 source 命令（推荐）：
    ```sql
    source sql/schema.sql;
    ```

3.  **修改 MySQL 密码以便 Python 连接** (重要！)：
    为了让 `app.py` 能连上数据库，我们需要设置 root 用户的密码（假设设为 `123456`，请根据你自己想设的改）：
    ```sql
    ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY '123456';
    FLUSH PRIVILEGES;
    EXIT;
    ```
    *(注：如果 MySQL 版本较新，报错的话，去掉 `WITH mysql_native_password` 试试)*

4.  **生成数据**：
    回到终端（Terminal），运行：
    ```bash
    python3 scripts/generate_data.py
    ```
    这会在 `sql/` 目录下生成 `data.sql`。

5.  **导入数据**：
    将生成的数据导入 MySQL（此时需要输入刚才设置的密码 `123456`）：
    ```bash
    mysql -u root -p < sql/data.sql
    ```
    *(输入命令后回车，然后输入密码)*

这一步实际配置时，如果在用conda，Conda 环境里自带了一个 mysql 客户端，它的默认配置路径（/tmp/...）和 Ubuntu 系统自带的 MySQL 服务路径（/var/run/...）不一致。可以强制通过 IP 连接，不要用 socket 文件连接，而是强制通过 TCP/IP 网络连接来解决。加上 -h 127.0.0.1 参数：
mysql -u root -p -h 127.0.0.1 < sql/data.sql


#### 第三步：配置并运行 Web 应用

1.  **修改配置文件**：
    你需要编辑 `app/app.py`。可以使用 `nano` 编辑器：
    ```bash
    nano app/app.py
    ```
    找到 `db_config` 部分，把 `password` 改成你在第二步里设置的密码（例如 `'123456'`）。
    *   按 `Ctrl + O` 保存，按 `Enter` 确认，按 `Ctrl + X` 退出。

2.  **运行应用**：
    ```bash
    python3 app/app.py
    ```

3.  **访问网页**：
    *   在 Windows 的浏览器中输入：`http://localhost:5000`。WSL2 会自动把端口转发出来。
    *   使用管理员账号登录测试：`admin` / `admin123`。

#### 第四步：运行 MapReduce

这里分两种情况，取决于你是否真的安装了庞大的 Hadoop 环境。

**情况 A：你没有安装 Hadoop，只想模拟测试逻辑（推荐快速验证）**
MapReduce Streaming 的原理其实就是 `cat` (读取) -> `map` -> `sort` -> `reduce`。你可以用 Linux 管道模拟：

1.  先导出日志：
    ```bash
    python3 scripts/export_logs.py
    ```
2.  模拟运行（假设你的 map 和 reduce 脚本分别是 `mapper.py` 和 `reducer.py`）：
    ```bash
    # 注意：需要给脚本加执行权限
    chmod +x mapreduce/mapper.py
    chmod +x mapreduce/reducer.py
    
    # 模拟命令
    cat access.log | python3 mapreduce/mapper.py | sort | python3 mapreduce/reducer.py
    ```
    如果你能看到统计结果，说明代码逻辑没问题。

**情况 B：你必须使用真实的 `hadoop` 命令**
这需要你已经在 WSL2 中下载并配置好了 Hadoop（这通常需要额外的复杂配置，包括设置 JAVA_HOME, 下载 Hadoop 二进制包等）。

如果你已经配好了 Hadoop，命令如下：
1.  启动 Hadoop (HDFS/YARN): `start-all.sh` (视你安装方式而定)。
2.  上传文件到 HDFS:
    ```bash
    hadoop fs -put access.log /input/
    ```
3.  运行 Streaming Jar (路径需根据你的 Hadoop 版本调整):
    ```bash
    hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-*.jar \
    -file mapreduce/mapper.py    -mapper "python3 mapper.py" \
    -file mapreduce/reducer.py   -reducer "python3 reducer.py" \
    -input /input/access.log \
    -output /output
    ```

### 总结 WSL2 特别注意事项

1.  **文件路径**：尽量把项目放在 Linux 的文件系统中（例如 `~/project`），而不要放在 `/mnt/c/Users/...` 下，否则 Python 读取文件和 MySQL 的速度会非常慢。
2.  **MySQL 服务**：每次重启 WSL2 后，MySQL 服务可能会停止。如果连接不上数据库，记得运行 `sudo service mysql start`。
3.  **localhost**：如果在 Windows 浏览器访问 `localhost:5000` 失败，尝试查看 WSL 的 IP 地址（在 WSL 输入 `ip addr`），然后通过 IP 访问，或者检查 Windows 防火墙。


### Hadoop安装

既然你要**最简单**的，那就选 **“单机模式 (Standalone Mode)”**。

这种模式**不需要**配置 SSH，**不需要**改 5 个 XML 文件，**不需要**启动服务，**不需要** `start-all.sh`。Hadoop 就像一个普通的软件一样直接运行。

以下是极简步骤：

### 第一步：安装 Java (必装)

打开 WSL 终端，复制粘贴运行：

```bash
sudo apt update
sudo apt install openjdk-8-jdk -y
```

### 第二步：下载并解压 Hadoop

直接复制粘贴运行（下载到当前目录）：

```bash
wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
tar -zxvf hadoop-3.3.6.tar.gz
mv hadoop-3.3.6 hadoop
```

/home/roujin/Python/hadoop

### 第三步：告诉 Hadoop Java 在哪 (唯一需要改的配置)

运行这行命令，直接把配置写入文件（免去你打开编辑器找位置的麻烦）：

```bash
echo "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64" >> /home/roujin/Python/hadoop/etc/hadoop/hadoop-env.sh
```

---

### 第四步：怎么运行你的项目？

**注意：** 因为是单机模式，不需要上传文件到 HDFS，**直接读取你本地的文件**。

请完全忽略你项目 README 里的 `start-dfs.sh` 和 `hadoop fs -put` 步骤。

**直接运行计算命令：**

假设你已经在项目根目录，且生成了 `access.log`。

**任务 1 的运行命令（修改版）：**
*(注意：我把输入输出路径改成了本地路径，请直接复制)*

```bash
/home/roujin/Python/hadoop/bin/hadoop jar /home/roujin/Python/hadoop/share/hadoop/tools/lib/hadoop-streaming-*.jar \
    -input access.log \
    -output output_task1 \
    -mapper "python3 mapreduce/mapper_1.py" \
    -reducer "python3 mapreduce/reducer_1.py" \
    -file mapreduce/mapper_1.py \
    -file mapreduce/reducer_1.py
```

**运行成功后查看结果：**
```bash
cat output_task1/part-00000
```

---

### 总结

1.  装 Java。
2.  下 Hadoop。
3.  设 JAVA_HOME。
4.  直接用 `~/hadoop/bin/hadoop ...` 跑命令。

这就是宇宙最简方案。